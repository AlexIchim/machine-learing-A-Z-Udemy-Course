print("Hello World")
cd ..
ls
file:///C:/Users/TekAdvice/Desktop/Machine Learning A-Z Template Folder/Part 1 - Data Preprocessing/Section 2 -------------------- Part 1 - Data Preprocessing --------------------/MyPreProcessing/Data.csv
file:///C:/Users/TekAdvice/Desktop/Machine Learning A-Z Template Folder/Part 1 - Data Preprocessing/Section 2 -------------------- Part 1 - Data Preprocessing --------------------/MyPreProcessing/data_preprocessing_template.py
file:///C:/Users/TekAdvice/Desktop/Machine Learning A-Z Template Folder/Part 1 - Data Preprocessing/Section 2 -------------------- Part 1 - Data Preprocessing --------------------/MyPreProcessing/untitled1.py
system("python \"C:/Users/TekAdvice/Desktop/Machine Learning A-Z Template Folder/Part 1 - Data Preprocessing/Section 2 -------------------- Part 1 - Data Preprocessing --------------------/RPreProcessing/data_preprocessing_template.py\"")
dataset = read.csv('Data.csv')
ls
setwd("C:/Users/TekAdvice/Desktop/Desktop/Machine Learning A-Z Template Folder/Part 4 - Clustering/Section 24 - K-Means Clustering")
dataset = read.csv('Mall_Customers.csv')
X <- dataset[4:5]
# Using the elbow method to find the option number of clusters
set.seed(6)
wcss <- vector()
for (i in 1:10) wcss[i] <- sum(kmeans(X, i)$withinss)
plot(1:10, wcs, type = "b",
main = paste('Clusters of clients'),
xlab = "Number of clusters",
ylab = "WCSS")
# K-Means Clustering
#Importing the mall dataset
dataset = read.csv('Mall_Customers.csv')
X <- dataset[4:5]
# Using the elbow method to find the option number of clusters
set.seed(6)
wcss <- vector()
for (i in 1:10) wcss[i] <- sum(kmeans(X, i)$withinss)
plot(1:10, wcss, type = "b",
main = paste('Clusters of clients'),
xlab = "Number of clusters",
ylab = "WCSS")
# Applying k-means to the mall dataset
set.seed(29)
kmeans <- kmeans(X, 5, iter.max = 300, nstart = 10)
library(cluster)
library(cluster)
clusplot(X,
kmeans$cluster,
lines = 0,
shade = TRUE,
color = TRUE,
labels = 2,
plotchar = FALSE
span = TRUE,
main = paste('Cluster of clients'),
xlab = "Annual Income",
ylabe = "Spending Score")
library(cluster)
clusplot(X,
kmeans$cluster,
lines = 0,
shade = TRUE,
color = TRUE,
labels = 2,
plotchar = FALSE,
span = TRUE,
main = paste('Cluster of clients'),
xlab = "Annual Income",
ylabe = "Spending Score")
setwd("C:/Users/TekAdvice/Desktop/Desktop/Machine Learning A-Z Template Folder/Part 4 - Clustering/Section 25 - Hierarchical Clustering")
clear
setwd("C:/Users/TekAdvice/Desktop/Desktop/Machine Learning A-Z Template Folder/Part 4 - Clustering/Section 25 - Hierarchical Clustering")
# Importing the dataset
dataset = read.csv('Mall_Customers.csv')
X = dataset[4:5]
# Using the dendogram to find the optimal number of clusters
dendogram = hclust(dist(X, method = 'euclidean'), mthod = 'ward.D')
plot(dendogram,
main = paste('Dendogram'),
xlab = 'Customers',
ylab = 'Euclidean distances')
# Hierarchical Clustering
# Importing the dataset
dataset = read.csv('Mall_Customers.csv')
X = dataset[4:5]
dendogram = hclust(dist(X, method = 'euclidean'), mthod = 'ward.D')
plot(dendogram,
main = paste('Dendogram'),
xlab = 'Customers',
ylab = 'Euclidean distances')
dendogram = hclust(dist(X, method = 'euclidean'), mthod = 'ward.D')
dendogram = hclust(dist(X, method = 'euclidean'), method = 'ward.D')
plot(dendogram,
main = paste('Dendogram'),
xlab = 'Customers',
ylab = 'Euclidean distances')
# Fitting hierarchical clustering to the mall dataset
hc = hclust(dist(X, method = 'euclidean'), method = 'ward.D')
y_hc = cutree(hc, 5)
y_hc
library(cluster)
clusplot(X,
y_hc,
lines = 0,
shade = TRUE,
color = TRUE,
labels = 2,
plotchar = FALSE,
span = TRUE,
main = paste('Cluster of clients'),
xlab = "Annual Income",
ylabe = "Spending Score")
